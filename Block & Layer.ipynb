{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61b36660",
   "metadata": {},
   "source": [
    "层和块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1292b596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0095, -0.2507, -0.1657, -0.2215, -0.0809, -0.2350,  0.0215,  0.1097,\n",
       "          0.1820,  0.0255,  0.1924,  0.0737, -0.0146,  0.0341, -0.1640,  0.1673,\n",
       "          0.0681,  0.3388, -0.1606, -0.1217],\n",
       "        [-0.0370, -0.2171, -0.1008, -0.3321, -0.0828, -0.2430,  0.1802,  0.1039,\n",
       "         -0.0426,  0.2624,  0.1158, -0.0192, -0.0720,  0.1906, -0.1109, -0.0510,\n",
       "          0.1033,  0.1217, -0.1282, -0.1730]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "net = nn.Sequential(nn.Linear(20,256),nn.ReLU(),nn.Linear(256,20))\n",
    "X = torch.rand(2,20)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e02a1320",
   "metadata": {},
   "outputs": [],
   "source": [
    "#自定义块\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()        \n",
    "        self.hidden = nn.Linear(20,256)\n",
    "        self.out = nn.Linear(256,10)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        '''前向计算'''\n",
    "        return self.out(F.relu(self.hidden(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c69bdac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1587, -0.1480, -0.1731,  0.0841, -0.3282, -0.1045,  0.0688,  0.0384,\n",
       "         -0.0055,  0.1775],\n",
       "        [-0.0175, -0.0286, -0.0803,  0.1117, -0.1632, -0.0053,  0.0393, -0.0191,\n",
       "         -0.0250,  0.0587]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e92120d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0947, -0.1717,  0.2374,  0.0767,  0.2126,  0.0640, -0.1982, -0.0414,\n",
       "          0.0391,  0.0869],\n",
       "        [ 0.0328, -0.2523,  0.3707, -0.0991,  0.3434,  0.1125, -0.1522,  0.0098,\n",
       "         -0.0897,  0.0461]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#顺序块\n",
    "class MySequential(nn.Module):\n",
    "    def __init__(self,*args):\n",
    "        super().__init__()\n",
    "        for layer in args:\n",
    "            self._modules[layer] = layer\n",
    "            \n",
    "    def forward(self,X):\n",
    "        for layer in self._modules.values():\n",
    "            X = layer(X)\n",
    "        return X\n",
    "\n",
    "net = MySequential(nn.Linear(20,256),nn.ReLU(),nn.Linear(256,10))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3d16403",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #不计算梯度的随机权重参数，因此其在训练期间保持不变\n",
    "        self.rand_weight = torch.rand((20,20),requires_grad=True)\n",
    "        self.linear = nn.Linear(20,20)\n",
    "        \n",
    "    def forward(self,X):\n",
    "        X = self.linear(X)\n",
    "        #使用创建的常量参数以及relu和mm函数\n",
    "        X = F.relu(torch.mm(X,self.rand_weight) + 1)\n",
    "        #复用全连接层，这相当于两个全连接层共享参数\n",
    "        X = self.linear(X)\n",
    "        #控制流\n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "687d6c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0872, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = FixedHiddenMLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff8ccd6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2553, 0.0915, 0.0000, 0.0000, 0.0636, 0.0000, 0.2931, 0.1515, 0.0405,\n",
       "         0.0000, 0.0000, 0.0000, 0.0225, 0.0000, 0.0000, 0.0627, 0.0000, 0.0000,\n",
       "         0.0000, 0.2936],\n",
       "        [0.2807, 0.1030, 0.0000, 0.0000, 0.0680, 0.0000, 0.3305, 0.1605, 0.0277,\n",
       "         0.0000, 0.0000, 0.0000, 0.0402, 0.0000, 0.0000, 0.0884, 0.0000, 0.0000,\n",
       "         0.0000, 0.3104]], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#混合搭配各种组合块\n",
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(20,64),nn.ReLU(),\n",
    "                                nn.Linear(64,32),nn.ReLU())\n",
    "        self.linear = nn.Linear(32,16)\n",
    "        \n",
    "    def forward(self,X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "chimera = nn.Sequential(NestMLP(),nn.Linear(16,20),nn.ReLU())\n",
    "chimera(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff388f26",
   "metadata": {},
   "source": [
    "参数管理\n",
    "--访问参数，用于调试、诊断和可视化\n",
    "--参数初始化\n",
    "--在不同模组间共享参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd1f040a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2582],\n",
       "        [0.3373]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "net = nn.Sequential(nn.Linear(4,8),nn.ReLU(),nn.Linear(8,1))\n",
    "X = torch.rand(size=(2,4))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312af618",
   "metadata": {},
   "source": [
    "参数访问"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "206c39cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[ 0.0095, -0.2743, -0.0914, -0.0427,  0.0271,  0.2213,  0.2706,  0.2373]])), ('bias', tensor([0.2871]))])\n"
     ]
    }
   ],
   "source": [
    "#参数访问\n",
    "print(net[2].state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2557c2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([0.2871], requires_grad=True)\n",
      "tensor([0.2871])\n"
     ]
    }
   ],
   "source": [
    "#目标参数\n",
    "print(type(net[2].bias))\n",
    "print(net[2].bias)\n",
    "print(net[2].bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d852769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([ 0.4901, -0.4835,  0.3613, -0.0267, -0.4491,  0.1956, -0.2597, -0.1758],\n",
      "       requires_grad=True)\n",
      "tensor([ 0.4901, -0.4835,  0.3613, -0.0267, -0.4491,  0.1956, -0.2597, -0.1758])\n"
     ]
    }
   ],
   "source": [
    "print(type(net[0].bias))\n",
    "print(net[0].bias)\n",
    "print(net[0].bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ada691f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#参数是复合的对象，包含值，梯度和额外信息\n",
    "#上述过程中还未调用反向传播，所以参数的梯度处于初始状态\n",
    "net[2].weight.grad == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "061e8cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))\n",
      "('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))\n"
     ]
    }
   ],
   "source": [
    "#一次性访问所有参数\n",
    "print(*[(name,param.shape) for name,param in net[0].named_parameters()])\n",
    "print(*[(name,param.shape) for name,param in net.named_parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93aca736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2871])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()['2.bias'].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3df8f4ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2222],\n",
       "        [-0.2219]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#从嵌套块收集参数\n",
    "def block1():\n",
    "    return nn.Sequential(nn.Linear(4,8),nn.ReLU(),nn.Linear(8,4),nn.ReLU())\n",
    "\n",
    "def block2():\n",
    "    net = nn.Sequential()\n",
    "    #在这里嵌套\n",
    "    for i in range(4):\n",
    "        net.add_module(f'block{i}',block1())\n",
    "    return net\n",
    "\n",
    "rgnet = nn.Sequential(block2(),nn.Linear(4,1))\n",
    "rgnet(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45729570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (block0): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block1): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block2): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block3): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (1): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(rgnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339a38c6",
   "metadata": {},
   "source": [
    "参数初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91801f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0098,  0.0016,  0.0185, -0.0122]), tensor(0.))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#内置初始化\n",
    "def init_normal(m):\n",
    "    '''正态分布初始化'''\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight,mean=0,std=0.01) #均值为0，方差为0.01\n",
    "        nn.init.zeros_(m.bias)\n",
    "net.apply(init_normal)\n",
    "net[0].weight.data[0],net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb9df4b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1.]), tensor(0.))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " def init_constant(m):\n",
    "        '''初始化为常数'''\n",
    "        if type(m) == nn.Linear:\n",
    "            nn.init.constant_(m.weight,1)\n",
    "            nn.init.zeros_(m.bias)\n",
    "net.apply(init_constant)\n",
    "net[0].weight.data[0],net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8c51beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1244,  0.0347, -0.6821,  0.7056],\n",
      "        [-0.0490, -0.6878,  0.0857, -0.5249],\n",
      "        [ 0.3420,  0.4452, -0.5306,  0.6095],\n",
      "        [ 0.4234, -0.2931, -0.6981, -0.4768],\n",
      "        [ 0.0982,  0.3584,  0.0026,  0.5649],\n",
      "        [ 0.0263, -0.2315,  0.5587,  0.6546],\n",
      "        [-0.4286, -0.1850, -0.6635, -0.1223],\n",
      "        [ 0.1545,  0.6119, -0.4049, -0.4272]])\n",
      "tensor([[22., 22., 22., 22., 22., 22., 22., 22.]])\n"
     ]
    }
   ],
   "source": [
    "#对某些块应用不同的初始化方法\n",
    "def xavier(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        \n",
    "def init_22(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight,22)\n",
    "\n",
    "net[0].apply(xavier)\n",
    "net[2].apply(init_22)\n",
    "print(net[0].weight.data)\n",
    "print(net[2].weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "249a41f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init torch.Size([8, 4]) weight\n",
      "Init torch.Size([1, 8]) weight\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  7.7449,  0.0000, -0.0000],\n",
       "        [-8.1785,  8.1991, -5.6476, -0.0000]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#自定义初始化\n",
    "def my_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        print(\n",
    "            \"Init\",\n",
    "            *[{name,param.shape} for name,param in m.named_parameters()][0])\n",
    "        nn.init.uniform_(m.weight,-10,10)\n",
    "        m.weight.data *= m.weight.data.abs() >= 5\n",
    "\n",
    "net.apply(my_init)\n",
    "net[0].weight.data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92e20c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True, True, True])\n",
      "tensor([True, True, True, True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "#参数绑定\n",
    "shared = nn.Linear(8,8)\n",
    "net = nn.Sequential(nn.Linear(4,8),nn.ReLU(),shared,\n",
    "                    nn.ReLU(),shared,nn.ReLU(),nn.Linear(8,1))\n",
    "net(X)\n",
    "#2,4隐藏层共享权重--检查参数是否相同\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])\n",
    "net[2].weight.data[0,0] = 100\n",
    "#实际上他们是同一个对象\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dcd555",
   "metadata": {},
   "source": [
    "自定义层 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c09fb09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#不带参数的层\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "class CenteredLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self,X):\n",
    "        return X - X.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71981d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2., -1.,  0.,  1.,  2.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#自行提供数据\n",
    "layer = CenteredLayer()\n",
    "layer(torch.FloatTensor([1,2,3,4,5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40e933b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#组合到更复杂的模型\n",
    "net = nn.Sequential(nn.Linear(8,128),CenteredLayer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0c2ba2c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-3.7253e-09, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = net(torch.rand(4,8))\n",
    "Y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "000b2468",
   "metadata": {},
   "outputs": [],
   "source": [
    "#带参数的层\n",
    "class MyLinear(nn.Module):\n",
    "    def __init__(self,in_units,units):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(in_units,units))\n",
    "        self.bias = nn.Parameter(torch.randn(units,))\n",
    "    def forward(self,X):\n",
    "        linear = torch.matmul(X,self.weight.data) + self.bias.data\n",
    "        return F.relu(linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b3849047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0399, -1.0321, -1.9512],\n",
       "        [-0.2620, -0.3568, -0.3937],\n",
       "        [ 1.1930, -0.8624, -0.1014],\n",
       "        [-1.0170,  1.5000,  0.0909],\n",
       "        [-0.7188,  1.8407,  0.0103]], requires_grad=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = MyLinear(5,3)\n",
    "linear.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5423e40a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.4638, -1.0623,  0.4178], requires_grad=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ae74adc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear(torch.rand(2,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bd366af7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.3124],\n",
       "        [5.0040]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#还可以使用自定义构建模型，就像使用内置的全连接层一样使用自定义层\n",
    "net = nn.Sequential(MyLinear(64,8),MyLinear(8,1))\n",
    "net(torch.rand(2,64))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b36c238",
   "metadata": {},
   "source": [
    "读写文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d81c122f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载和保存张量\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "X= torch.arange(4)\n",
    "torch.save(X,'X-file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "92d02100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.load(\"X-file\")\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7a458bd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3]), tensor([1., 1., 1., 1.]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#存储一个张量列表，然后把他们读回内存\n",
    "y = torch.ones(4)\n",
    "torch.save([X,y],'X-file')\n",
    "x2,y2 = torch.load('X-file')\n",
    "(x2,y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4051d48e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X': tensor([0, 1, 2, 3]), 'y': tensor([1., 1., 1., 1.])}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#写入或读取从字符串映射到张量的字典\n",
    "mydict = {'X':X,'y':y}\n",
    "torch.save(mydict,'mydict')\n",
    "mydict2 = torch.load('mydict')\n",
    "mydict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b42a9e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载和保存模型参数\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20,256)\n",
    "        self.output = nn.Linear(256,10)\n",
    "        \n",
    "    def forward(self,X):\n",
    "        return self.output(F.relu(self.hidden(X)))\n",
    "    \n",
    "net = MLP()\n",
    "X = torch.randn(size=(2,20))\n",
    "Y = net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f24fe27c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1554,  0.1887, -0.1701, -0.2092, -0.6236, -0.0218,  0.1294,  0.1631,\n",
       "          0.0543,  0.0417],\n",
       "        [ 0.1640,  0.2294,  0.0881,  0.0970, -0.4318, -0.2058,  0.0543, -0.4190,\n",
       "          0.1111, -0.0534]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1306ae15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#将模型参数存储，放置于'mlp.params'\n",
    "torch.save(net.state_dict(),'mlp.params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9a6266dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (hidden): Linear(in_features=20, out_features=256, bias=True)\n",
       "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#为了恢复模型，实例化一个原始多层感知机的备份，不初始化参数，直接读取备份参数\n",
    "clone = MLP()\n",
    "clone.load_state_dict(torch.load('mlp.params'))\n",
    "clone.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4c49ff15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_clone = clone(X)\n",
    "Y_clone == Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fcd73da6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cpu'), device(type='cuda'), device(type='cuda', index=1))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "19bd76b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d0429c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
